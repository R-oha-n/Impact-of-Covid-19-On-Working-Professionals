{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11255968,"sourceType":"datasetVersion","datasetId":7034489}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Classification and Regression Tree","metadata":{}},{"cell_type":"markdown","source":"$\\underline{Problem Statement}$: Given the Independent variables, we have to perform classification on the basis of job_security as target variable.","metadata":{}},{"cell_type":"code","source":"# Importing the necessary Modules\nimport pandas as pd   \nimport numpy as np   \nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"ExecuteTime":{"end_time":"2020-03-25T07:48:10.524490Z","start_time":"2020-03-25T07:48:10.520540Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.getcwd() ","metadata":{"ExecuteTime":{"end_time":"2020-03-25T07:48:10.662123Z","start_time":"2020-03-25T07:48:10.526485Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(\"C:/Users/KIIT/Downloads/covid_impact_on_work new.csv\")\ndf.head()","metadata":{"ExecuteTime":{"end_time":"2020-03-25T07:48:10.841644Z","start_time":"2020-03-25T07:48:10.664119Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let us now check for the missing values.","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"ExecuteTime":{"end_time":"2020-03-25T07:48:10.932433Z","start_time":"2020-03-25T07:48:10.843637Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"There are no missing values in the dataset and we can go ahead with building the model.","metadata":{}},{"cell_type":"markdown","source":"Let us check the number of rows and the number of columns in the dataframe.","metadata":{}},{"cell_type":"code","source":"print('The number of rows (observations) is:',df.shape[0],'\\n''The number of columns(variables) is:',df.shape[1])","metadata":{"ExecuteTime":{"end_time":"2020-03-25T07:48:11.056105Z","start_time":"2020-03-25T07:48:10.933397Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## We will drop the 'Sector','Childcare_Responsibilities','Team_Collaboration_Challenges' variable.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df=df.drop(['Sector','Childcare_Responsibilities','Team_Collaboration_Challenges'], axis=1)\ndf.head()","metadata":{"ExecuteTime":{"end_time":"2020-03-25T07:48:11.177771Z","start_time":"2020-03-25T07:48:11.057066Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let us check the data types of each of the variables in the data.","metadata":{}},{"cell_type":"code","source":"df.dtypes","metadata":{"ExecuteTime":{"end_time":"2020-03-25T07:48:11.346294Z","start_time":"2020-03-25T07:48:11.180736Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"There are three variables (Stress_Level, Hours_Worked_Per_Day & Meetings_Per_Day) which has the object data type.\n\nsklearn in Python does not take the input of object data types when building Classification Trees. So, we need to convert these variables into some numerical form.","metadata":{}},{"cell_type":"markdown","source":"We have a choice of converting objects into categories if there are only three levels in a variable like Stress Level (Low / Medium / High) or if the data type is supposedly ordinal in nature whereby assigning numbers will represent their corresponding weightage.\n\nThe category data type in pandas is a $\\underline{hybrid}$ data type. It looks and behaves like a string in many instances but internally is represented by an array of integers. This way, Python will treat it as a numerical variable.\n\n\nIf this is not the case where the catagory is nominal, One hot encoding is the recommended way forward.","metadata":{}},{"cell_type":"markdown","source":"The following code is provided to you to convert the 'object' type variables into categories(Hybrid data types) to numerical variables by assigning ranks/numbers to each category. Though we are not using it here in this case. Our Categorical variables have multiple levels and therefore \"One hot encoding it is\"","metadata":{}},{"cell_type":"code","source":"#We could use the following code snippet in the loop. \n#df['Stress_Level']=pd.Categorical(df['Stress_Level']).codes #code used for assigning numerical value to each category","metadata":{"ExecuteTime":{"end_time":"2020-03-25T07:48:11.518868Z","start_time":"2020-03-25T07:48:11.348323Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#We are prepping the data by segregating them into Target and independent variables to runt his model going forward\n\n# Let us define the X(predictor) and Y(target) variables\n\nX = df.drop(\"Job_Security\" , axis=1)\n\nY = df.pop(\"Job_Security\")\n#we have made a copy of the data frame as the 'pop' function removes that particular variable from the data frame and stores \n# in another variable","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This line of code is to perform one hot encoding for Categorical Features\nX = pd.get_dummies( X, drop_first = False )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"list(X.columns)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X.dtypes","metadata":{"ExecuteTime":{"end_time":"2020-03-25T07:48:11.924784Z","start_time":"2020-03-25T07:48:11.795094Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We see that the data types of all the variables have been changed to either of $\\underline{int64}$ or $\\underline{bool}$.","metadata":{}},{"cell_type":"code","source":"#You shall notice One hot encoding converts catagorical variable (Levels) into seperate columns with binary values \nX.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Split the data into Train and Test.","metadata":{}},{"cell_type":"markdown","source":"Before building the model we should split the data into Train and Test. We will thus build a model on the training data and use this model to predict on the test data.","metadata":{}},{"cell_type":"markdown","source":"We will be doing a 70:30 split.\n70% of the whole data will be used to train the data and then 30% of the data will be used for testing the model thus built.","metadata":{}},{"cell_type":"markdown","source":"Before splitting the data, we shall make a copy of the data frame.","metadata":{}},{"cell_type":"code","source":"data = df.copy()\ndata.head()\n#Just keepipng the copy of the original dataset.","metadata":{"ExecuteTime":{"end_time":"2020-03-25T07:48:12.056779Z","start_time":"2020-03-25T07:48:11.926741Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.30, random_state=1)","metadata":{"ExecuteTime":{"end_time":"2020-03-25T07:48:12.346211Z","start_time":"2020-03-25T07:48:12.228320Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now that we have split the data into Train and Test, let us go ahead and build our Decision Tree Model.","metadata":{"ExecuteTime":{"end_time":"2020-03-24T17:15:36.610145Z","start_time":"2020-03-24T17:15:36.601121Z"}}},{"cell_type":"markdown","source":"## Building the Decision Tree","metadata":{}},{"cell_type":"markdown","source":"We will start by building a very basic Decision Tree model.","metadata":{}},{"cell_type":"code","source":"from sklearn import tree","metadata":{"ExecuteTime":{"end_time":"2020-03-25T07:48:12.477628Z","start_time":"2020-03-25T07:48:12.347945Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dt_model = tree.DecisionTreeClassifier(criterion = 'gini',random_state=1)","metadata":{"ExecuteTime":{"end_time":"2020-03-25T07:48:12.646697Z","start_time":"2020-03-25T07:48:12.479578Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In the above code snippet we have defined a Decision Tree (which is to be used for classification problems) with the splitting criteria for each node as 'gini'. The 'random_state' parameter ensures that each time we run the code snippet the values remains the same. \n\nIn the above code snippet default values of 'min_samples_split' and 'min_samples_leaf' is taken as 2 and 1 respectively.","metadata":{}},{"cell_type":"markdown","source":"Now, that we have defined a Decision Tree, let us go ahead and build the model on the training data.","metadata":{}},{"cell_type":"code","source":"dt_model.fit(X_train, Y_train)","metadata":{"ExecuteTime":{"end_time":"2020-03-25T07:48:12.895175Z","start_time":"2020-03-25T07:48:12.648739Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualizing the Decision Tree","metadata":{}},{"cell_type":"markdown","source":"Now, that we have built the tree let us go ahead and visualize the tree to understand the various nuances of the Classification Tree that we just built.","metadata":{}},{"cell_type":"code","source":"train_char_label = ['No', 'Yes']# defining the classes of the target variable for the ease","metadata":{"ExecuteTime":{"end_time":"2020-03-25T07:48:13.005215Z","start_time":"2020-03-25T07:48:12.897144Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, we need to create a dot file which will contains all the instructions on how build this graphical visualization of the Classification Tree that we had built.","metadata":{}},{"cell_type":"code","source":"os.getcwd()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dot_data = tree.export_graphviz(dt_model, #passing the model that we had built earlier\n                                feature_names = list(X_train), #names of the independent variables\n                                class_names = list(train_char_label)) #passing the names of the classes that we had defined\n","metadata":{"ExecuteTime":{"end_time":"2020-03-25T07:48:13.426432Z","start_time":"2020-03-25T07:48:13.011152Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#If the below two libraries are not installed do install them using the following code snippet in the Jupyter Notebook\n# !pip install 'package name'\nimport pydotplus\nimport graphviz","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Optional - Install only if the prev code throws error else skip running this line\n!pip install pydotplus","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Optional - Install only if the prev code throws error else skip running this line\n!pip install graphviz","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dot_data = tree.export_graphviz(dt_model, out_file=None,  #passing the model built and setting the output to None as we do\n                                #not need the dot file separately to visualize the graph\n                         feature_names=list(X_train),    #names of the independent variables \n                         class_names=list(train_char_label),  \n                         filled=True)                    #colours the nodes for classification for the ease of visualization\n\ngraph = pydotplus.graph_from_dot_data(dot_data) #extracting the visuals from the above file to plot it","metadata":{"ExecuteTime":{"end_time":"2020-03-25T07:48:30.096523Z","start_time":"2020-03-25T07:48:13.434339Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let us check importance of the variables in the Classification Tree that we just built. The importance of a feature or variable is computed as the (normalized) total reduction of the gini criterion brought by that feature. It is also known as the Gini importance. ","metadata":{}},{"cell_type":"code","source":"pd.Series(dt_model.feature_importances_,index=X_train.columns).sort_values(ascending=False)","metadata":{"ExecuteTime":{"end_time":"2020-03-25T07:48:48.235659Z","start_time":"2020-03-25T07:48:48.226686Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"From the above output, we can see that 'Health_Issue' is the most important variable followed by 'Meetings_Per_Day_5.207.623.357.656.770' and so on.","metadata":{}},{"cell_type":"markdown","source":"Let us take a look at the overall accuracy of the train and test data using the model that we just built.","metadata":{}},{"cell_type":"code","source":"#Train Data\ndt_model.score(X_train,Y_train)","metadata":{"ExecuteTime":{"end_time":"2020-03-25T07:48:48.351618Z","start_time":"2020-03-25T07:48:48.236657Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Test Data\ndt_model.score(X_test,Y_test)","metadata":{"ExecuteTime":{"end_time":"2020-03-25T07:48:48.503559Z","start_time":"2020-03-25T07:48:48.352581Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The accuracy on the Training Data is 100% and the accuracy on the Test Data is lesser substantially. The model has surely been overfitted. \nThus, we need to prune or regularize the tree.","metadata":{}},{"cell_type":"markdown","source":"## Pruning/Regularizing the Tree","metadata":{}},{"cell_type":"markdown","source":"For Pruning/Regularizing the Tree we need to be sure as to what parameters and how to prune the tree.","metadata":{"ExecuteTime":{"end_time":"2020-03-25T06:53:51.659514Z","start_time":"2020-03-25T06:53:51.654527Z"}}},{"cell_type":"markdown","source":"# Method 1 for Pruning:\n\n#### (by visualizing the tree)","metadata":{}},{"cell_type":"code","source":"reg_dt_model = tree.DecisionTreeClassifier(criterion = 'gini', \n                                       max_depth=13,         #upto this depth is where the tree has grown uniformly\n                                      min_samples_leaf=30,   #ensures that every terminal node (leaf node) have at least 10\n                                      #observations in it\n                                      min_samples_split=10)  #for every node to be split into two child nodes that particular\n                                      #node should have at least 30 observations  \n    \n#Genral Thumb rule: 1% to 3% of the data should be the 'min_samples_split' and one third times the 'min_samples_split'\n#should be 'min_samples_leaf.These are only a rough guideline value.\n#Here, we have chose 1% of our training data which has 10,000 observations (100 observations for min_samples_split)","metadata":{"ExecuteTime":{"end_time":"2020-03-25T07:48:48.632859Z","start_time":"2020-03-25T07:48:48.505518Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"reg_dt_model.fit(X_train, Y_train)","metadata":{"ExecuteTime":{"end_time":"2020-03-25T07:48:48.815308Z","start_time":"2020-03-25T07:48:48.634855Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now that we have built the Pruned/Regularized Classification Tree let us visualize the tree to understand the nuances of the tree.","metadata":{}},{"cell_type":"markdown","source":"### Method 1:","metadata":{}},{"cell_type":"code","source":"# Define the path of the dot.file from the output of this code below\nos.getcwd()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dot_data = tree.export_graphviz(reg_dt_model, #passing the model that we had built earlier\n                                feature_names = list(X_train), #names of the independent variables\n                                class_names = list(train_char_label)) #passing the names of the classes that we had defined","metadata":{"ExecuteTime":{"end_time":"2020-03-25T07:48:48.940237Z","start_time":"2020-03-25T07:48:48.816305Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Method 2:","metadata":{}},{"cell_type":"code","source":"#The following is a modified version of the above code where we tried create a dot file and visualize\n\ndot_data = tree.export_graphviz(reg_dt_model, out_file=None,#passing the model built and setting the output to None as we do\n                                #not need the dot file separately to visualize the graph\n                         feature_names=list(X_train), #names of the independent variables \n                         class_names=list(train_char_label),  \n                         filled=True) #colours the nodes for classification for the ease of visualization\n\ngraph = pydotplus.graph_from_dot_data(dot_data) #extracting the visuals from the above file to plot it","metadata":{"ExecuteTime":{"end_time":"2020-03-25T07:48:51.981777Z","start_time":"2020-03-25T07:48:48.941188Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let us now go ahead and predict both the classes and the probability values on the test data using the Pruned/Regularized Decision Tree.","metadata":{}},{"cell_type":"code","source":"#We are only predicting the classes over here. Python by default takes the 0.5 cutoff of the probability values while\n#predicting the classes\n\nY_train_predict_class = reg_dt_model.predict(X_train)\nY_test_predict_class = reg_dt_model.predict(X_test)","metadata":{"ExecuteTime":{"end_time":"2020-03-25T07:52:00.510469Z","start_time":"2020-03-25T07:52:00.501450Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Here, we are predicting the probabilities and we can manually input a cutoff value which is different than 0.5.\n\nY_train_predict_prob = reg_dt_model.predict_proba(X_train)\nY_test_predict_prob = reg_dt_model.predict_proba(X_test)","metadata":{"ExecuteTime":{"end_time":"2020-03-25T07:53:32.721418Z","start_time":"2020-03-25T07:53:31.050847Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation of the Train and Test Models.","metadata":{}},{"cell_type":"markdown","source":"Let us first build the confusion matrix, followed by the Classification Report","metadata":{}},{"cell_type":"code","source":"from sklearn import metrics","metadata":{"ExecuteTime":{"end_time":"2020-03-25T07:55:07.194199Z","start_time":"2020-03-25T07:55:07.190246Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"First we will evaluate the model on the Training Data.","metadata":{"ExecuteTime":{"end_time":"2020-03-25T08:21:59.360236Z","start_time":"2020-03-25T08:21:59.355248Z"}}},{"cell_type":"code","source":"print(metrics.confusion_matrix(Y_train,Y_train_predict_class))","metadata":{"ExecuteTime":{"end_time":"2020-03-25T08:21:23.602872Z","start_time":"2020-03-25T08:21:23.394483Z"},"scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tn, fp, fn, tp = metrics.confusion_matrix(Y_train,Y_train_predict_class).ravel()\nprint('True Negative:',tn,'\\n''False Positives:' ,fp,'\\n''False Negatives:', fn,'\\n''True Positives:', tp)","metadata":{"ExecuteTime":{"end_time":"2020-03-25T08:22:29.312088Z","start_time":"2020-03-25T08:22:29.297094Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let us now go ahead and print the classification report to check the various other parameters.","metadata":{}},{"cell_type":"code","source":"print(metrics.classification_report(Y_train,Y_train_predict_class))","metadata":{"ExecuteTime":{"end_time":"2020-03-25T08:26:06.730702Z","start_time":"2020-03-25T08:26:06.518232Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We have been able to predict 60% of the target variables correctly.","metadata":{}},{"cell_type":"markdown","source":"Let us check the confusion matrix for the test data.","metadata":{}},{"cell_type":"code","source":"print(metrics.confusion_matrix(Y_test,Y_test_predict_class),'\\n')\ntn, fp, fn, tp = metrics.confusion_matrix(Y_test,Y_test_predict_class).ravel()\nprint('True Negative:',tn,'\\n''False Positives:' ,fp,'\\n''False Negatives:', fn,'\\n''True Positives:', tp)","metadata":{"ExecuteTime":{"end_time":"2020-03-25T08:28:20.788075Z","start_time":"2020-03-25T08:28:20.772113Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let us now go ahead and print the classification report for the test data and compare between train and test.","metadata":{}},{"cell_type":"code","source":"print(metrics.classification_report(Y_test,Y_test_predict_class))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Import necessary libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\n\n# Assuming 'df' is your dataset, define features and target\nX = df.drop(columns=['target_column'])  # Replace 'target_column' with the actual column name\ny = df['target_column']\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize and train Decision Tree Classifier\ndt_model = DecisionTreeClassifier(criterion='gini', random_state=1)\ndt_model.fit(X_train, y_train)\n\n# Make predictions\ny_train_pred = dt_model.predict(X_train)\ny_test_pred = dt_model.predict(X_test)\n\n# Compute accuracy\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\n\n# Plot Accuracy Graph\nplt.figure(figsize=(6, 4))\nplt.bar(['Train Accuracy', 'Test Accuracy'], [train_accuracy, test_accuracy], color=['blue', 'green'])\nplt.xlabel('Dataset')\nplt.ylabel('Accuracy Score')\nplt.title('Decision Tree Model Accuracy')\nplt.ylim(0, 1)\nplt.show()\n\n# Print Accuracy Scores\nprint(f\"Training Accuracy: {train_accuracy:.2f}\")\nprint(f\"Testing Accuracy: {test_accuracy:.2f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Ensure dataset (df) is loaded properly\n# Replace 'target_column' with actual target variable\nX = df.drop(columns=['target_column'])  \ny = df['target_column']\n\n# Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Store accuracy scores at different depths\ntrain_accuracies = []\ntest_accuracies = []\ndepths = range(1, 20)  # Testing depths from 1 to 20\n\n# Train model at different depths\nfor depth in depths:\n    dt_model = DecisionTreeClassifier(max_depth=depth, random_state=1)\n    dt_model.fit(X_train, y_train)\n\n    # Compute accuracy\n    train_acc = accuracy_score(y_train, dt_model.predict(X_train))\n    test_acc = accuracy_score(y_test, dt_model.predict(X_test))\n\n    train_accuracies.append(train_acc)\n    test_accuracies.append(test_acc)\n\n# Plot Accuracy Curve\nplt.figure(figsize=(8, 5))\nplt.plot(depths, train_accuracies, label=\"Training Accuracy\", marker='o', linestyle='-')\nplt.plot(depths, test_accuracies, label=\"Testing Accuracy\", marker='s', linestyle='--')\nplt.xlabel(\"Decision Tree Depth\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Model Accuracy vs. Tree Depth\")\nplt.legend()\nplt.grid()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}